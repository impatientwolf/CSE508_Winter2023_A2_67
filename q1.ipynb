{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Priyanshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Priyanshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Priyanshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Priyanshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocess\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_docs = []\n",
    "individual_tokens = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Index Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Priyanshu\\Downloads\\IR Ass2\\CSE508_Winter2023_Dataset'\n",
    "\n",
    "pos_index_dict = {}\n",
    "file_id_map_dict={}\n",
    "file_index = 1\n",
    "unprocessed_filelist = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "  \n",
    "  pos = 0\n",
    "\n",
    "  file_id_map_dict[filename]=file_index\n",
    "\n",
    "  try:\n",
    "    fp=open(os.path.join(path, filename),'r')\n",
    "\n",
    "    # tokens,preprocessed_text = preprocess(fp.read)\n",
    "    tokens=fp.read().split(\" \")\n",
    "    individual_tokens.append(tokens)\n",
    "    len_docs.append(len(tokens))\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "      if token in pos_index_dict:\n",
    "\n",
    "        pos_index_dict[token][0] += 1\n",
    "        \n",
    "        if file_index in pos_index_dict[token][1]:\n",
    "          pos_index_dict[token][2][file_index].append(pos)\n",
    "        else:\n",
    "          pos_index_dict[token][1].append(file_index)\n",
    "          pos_index_dict[token][2][file_index] = [pos]\n",
    "      \n",
    "      else:\n",
    "        pos_index_dict[token] = [1,[file_index],{file_index:[pos]}]\n",
    "      pos += 1\n",
    "\n",
    "  except:\n",
    "      unprocessed_filelist.append(file_index)\n",
    "\n",
    "  file_index += 1\n",
    "\n",
    "  with open('pos_index_dict','wb') as fd:\n",
    "    pickle.dump(pos_index_dict,fd)\n",
    "  with open('file_id_map_dict','wb') as fd:\n",
    "    pickle.dump(file_id_map_dict,fd)\n",
    "  with open('individual_tokens','wb') as fd:\n",
    "    pickle.dump(individual_tokens,fd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pos_index_dict','rb') as fd:\n",
    "#     pos_index_dict=pickle.load(fd)\n",
    "# with open('file_id_map_dict','rb') as fd:\n",
    "#     file_id_map_dict=pickle.load(fd)\n",
    "# with open('individual_tokens','rb') as fd:\n",
    "#     individual_tokens=pickle.load(fd)\n",
    "\n",
    "inv_file_id_map_dict={v: k for k, v in file_id_map_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = len(file_id_map_dict)\n",
    "vocab_size = len(pos_index_dict)\n",
    "vocab = list(pos_index_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrix(scheme):\n",
    "    if scheme=='binary':\n",
    "       from scheme import get_tf_binary as get_tf\n",
    "    elif scheme=='raw_count':\n",
    "       from scheme import get_tf_raw_count as get_tf\n",
    "    elif scheme=='term_frequency':\n",
    "       from scheme import get_tf_term_frequency as get_tf\n",
    "    elif scheme=='log_normalization':\n",
    "       from scheme import get_tf_logNorm as get_tf\n",
    "    else:\n",
    "       from scheme import get_tf_logDoubleNorm as get_tf\n",
    "        \n",
    "    matrix = np.empty((num_documents,vocab_size))\n",
    "\n",
    "    for i in range(num_documents):\n",
    "        for j in range(vocab_size):\n",
    "\n",
    "            curr_word = vocab[j]\n",
    "            doc_list = pos_index_dict[curr_word][1]\n",
    "            tf = 0\n",
    "\n",
    "            if i+1 in doc_list:\n",
    "                tf=get_tf(curr_word,i+1,pos_index_dict,len_docs,individual_tokens)\n",
    "\n",
    "            idf = math.log((num_documents*1.0)/(len(doc_list)+1),2)\n",
    "\n",
    "            matrix[i][j] = tf*idf \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix1 = createMatrix('binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix2 = createMatrix('raw_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix3 = createMatrix('term_frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix4 = createMatrix('log_normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix5 = createMatrix('double_normalization')\n",
    "with open('matrix5','wb') as fd:\n",
    "    pickle.dump(matrix5,fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('matrix5','rb') as fd:\n",
    "#     matrix5=pickle.load(fd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the query\n",
      "Yourr query is : removing finding doing stuff and forgetting who peoples are \\t\n",
      " Preprocessed query : ['removing', 'finding', 'stuff', 'forgetting', 'peoples']\n",
      "Unmatched Token Error : stuff\n",
      "Unmatched Token Error : forgetting\n",
      "Unmatched Token Error : peoples\n",
      "cranfield0738 7.129283016944966\n",
      "cranfield1323 5.910832407407448\n",
      "cranfield0745 5.3469622627087245\n",
      "cranfield1205 5.171978356481518\n",
      "cranfield0556 4.752855344629977\n"
     ]
    }
   ],
   "source": [
    "print(\"Input the query\")\n",
    "query = str(input())\n",
    "print(\"Yourr query is : \"+query)\n",
    "\n",
    "\n",
    "tokens,preprocessed_text = preprocess(query)\n",
    "print(\" Preprocessed query : \"+ str(tokens))\n",
    "indices = []\n",
    "doc_score = {}\n",
    "\n",
    "\n",
    "for t in tokens:\n",
    "    try:\n",
    "        indices.append(vocab.index(t))\n",
    "    except:\n",
    "        print(\"Unmatched Token Error : \"+str(t))\n",
    "\n",
    "for i in range(num_documents):\n",
    "  score = 0\n",
    "\n",
    "  for j in indices:\n",
    "    score = score + matrix5[i][j]\n",
    "  \n",
    "  doc_score[i+1] = score\n",
    "  \n",
    "marklist=reversed(sorted(doc_score.items(), key=lambda x:x[1]))\n",
    "sortdict = dict(marklist)\n",
    "\n",
    "answer = list(sortdict.keys())\n",
    "for i in range(5):\n",
    "  print(inv_file_id_map_dict[answer[i]], sortdict[answer[i]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input the phrase query\n",
      " Your query : solutions thethree particular casesi propose give general solutionto problem indicatebriefly obtained onedimensional transient heat temperature doublelayer slab triangular heatrate input one face insulated otherand thermal resistanceat interface flow multilayerslab recent contribution readersforum wassermann gave analyticsolutions \n",
      " Preprocessed query : ['solutions', 'thethree', 'particular', 'casesi', 'propose', 'give', 'general', 'solutionto', 'problem', 'indicatebriefly', 'obtained', 'onedimensional', 'transient', 'heat', 'temperature', 'doublelayer', 'slab', 'triangular', 'heatrate', 'input', 'one', 'face', 'insulated', 'otherand', 'thermal', 'resistanceat', 'interface', 'flow', 'multilayerslab', 'recent', 'contribution', 'readersforum', 'wassermann', 'gave', 'analyticsolutions']\n",
      "cranfield0006 0.7446808510638298\n",
      "cranfield0091 0.13333333333333333\n",
      "cranfield0582 0.11594202898550725\n",
      "cranfield0005 0.10526315789473684\n",
      "cranfield0981 0.08536585365853659\n"
     ]
    }
   ],
   "source": [
    "print(\"Input the phrase query\")\n",
    "query = str(input())\n",
    "print(\" Your query : \"+ str(query))\n",
    "tokens, preprocessed_text = preprocess(query)\n",
    "query = set(tokens)\n",
    "\n",
    "print(\" Preprocessed query : \"+ str(tokens))\n",
    "\n",
    "doc_score = {}\n",
    "\n",
    "for i in range(len(file_id_map_dict.keys())):\n",
    "\n",
    "  doc = set(individual_tokens[i])\n",
    "  num = len(list(doc & query))*1.0\n",
    "  den = len(list(doc | query))\n",
    "  doc_score[i] = num/den\n",
    "\n",
    "marklist = sorted(doc_score.items(), key=lambda x:x[1])\n",
    "sortdict = dict(reversed(marklist))\n",
    "\n",
    "answer = list(sortdict.keys())\n",
    "for i in range(5):\n",
    "  print(inv_file_id_map_dict[int(answer[i])+1], sortdict[answer[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
